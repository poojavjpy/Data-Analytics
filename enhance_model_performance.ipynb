{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoWr5iZNwVxukxJZGQAvDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojavjpy/Data-Analytics/blob/main/enhance_model_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and select features to enhance model performance for analytics tasks.**bold text**"
      ],
      "metadata": {
        "id": "1oafd49jPeHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOTQV3U4PD03",
        "outputId": "f486c0c2-b7b3-4241-ca0f-ba0d6d304151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating baseline (all features)...\n",
            "Baseline accuracy: 0.9649 +/- 0.0055\n",
            "\n",
            "Evaluating SelectKBest (f_classif), k=12...\n",
            "SelectKBest accuracy: 0.9543 +/- 0.0151\n",
            "\n",
            "SelectKBest selected features: ['mean radius', 'mean perimeter', 'mean area', 'mean concavity', 'mean concave points', 'worst radius', 'worst perimeter', 'worst area', 'worst concave points', 'radius_texture_interaction', 'perimeter_sq', 'log_area'] \n",
            "\n",
            "Evaluating RFE (LogisticRegression) selecting 10 features...\n",
            "RFE accuracy: 0.9508 +/- 0.0180\n",
            "\n",
            "RFE selected features: ['mean concave points', 'radius error', 'area error', 'compactness error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst concavity', 'worst concave points'] \n",
            "\n",
            "Evaluating SelectFromModel (RandomForest importance, threshold=median) ...\n",
            "SelectFromModel accuracy: 0.9561 +/- 0.0200\n",
            "\n",
            "SelectFromModel selected features: ['mean radius', 'mean perimeter', 'mean area', 'mean compactness', 'mean concavity', 'mean concave points', 'radius error', 'area error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst concavity', 'worst concave points', 'radius_texture_interaction', 'perimeter_sq', 'log_area'] \n",
            "\n",
            "\n",
            "Summary results:\n",
            "           method  mean_accuracy  std_accuracy\n",
            "     baseline_all       0.964850      0.005549\n",
            "select_from_model       0.956094      0.019962\n",
            "     select_kbest       0.954308      0.015084\n",
            "              rfe       0.950815      0.018015\n",
            "\n",
            "Notes:\n",
            "- Try recursive selection + hyperparameter tuning of the estimator for best results.\n",
            "- Try different feature creation ideas (domain-specific) and interaction terms.\n",
            "- For high-dimensional data, consider L1-based selection or dimensionality reduction (PCA).\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Feature engineering and feature selection demo (classification)\n",
        "- Uses sklearn breast cancer dataset\n",
        "- Creates new features (polynomial interactions, log transforms, bins)\n",
        "- Demonstrates preprocessing pipeline (impute, scale)\n",
        "- Compares model performance (cross-validation) for:\n",
        "  * baseline (all features)\n",
        "  * SelectKBest (f_classif)\n",
        "  * Recursive Feature Elimination (RFE)\n",
        "  * Model-based selection (RandomForest importance -> SelectFromModel)\n",
        "\n",
        "Run: python feature_engineering_and_selection.py\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer, KBinsDiscretizer, PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "def create_engineered_features(df):\n",
        "    \"\"\"Add a few engineered features to illustrate feature creation.\"\"\"\n",
        "    # 1. Interaction term: mean radius * mean texture\n",
        "    df['radius_texture_interaction'] = df['mean radius'] * df['mean texture']\n",
        "\n",
        "    # 2. Polynomial feature (square) of mean perimeter\n",
        "    df['perimeter_sq'] = df['mean perimeter'] ** 2\n",
        "\n",
        "    # 3. Log transform (safe) of mean area\n",
        "    df['log_area'] = np.log1p(df['mean area'])\n",
        "\n",
        "    # 4. Binned categorical from mean smoothness (3 bins)\n",
        "    kb = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
        "    df['smoothness_bin'] = kb.fit_transform(df[['mean smoothness']]).astype(int)\n",
        "\n",
        "    # 5. Ratio feature (concavity / concave points) with safe-guard\n",
        "    eps = 1e-6\n",
        "    df['concavity_over_concave_points'] = df['mean concave points'] / (df['mean concavity'] + eps)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_preprocessor(numeric_features):\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('impute', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', numeric_transformer, numeric_features)\n",
        "    ], remainder='drop')\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "def evaluate_pipeline(pipeline, X, y, cv):\n",
        "    scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "    return scores.mean(), scores.std()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load dataset\n",
        "    data = load_breast_cancer(as_frame=True)\n",
        "    X = data.frame.drop(columns=['target'])\n",
        "    y = data.target\n",
        "\n",
        "    # Start from original features\n",
        "    df = X.copy()\n",
        "\n",
        "    # Create engineered features\n",
        "    df = create_engineered_features(df)\n",
        "\n",
        "    # Feature list\n",
        "    all_features = list(df.columns)\n",
        "\n",
        "    # Preprocessor\n",
        "    preprocessor = build_preprocessor(all_features)\n",
        "\n",
        "    # Classifier used for evaluation (stable baseline)\n",
        "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "\n",
        "    # Baseline pipeline (all features)\n",
        "    pipeline_baseline = Pipeline(steps=[('pre', preprocessor), ('clf', clf)])\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    print('Evaluating baseline (all features)...')\n",
        "    baseline_mean, baseline_std = evaluate_pipeline(pipeline_baseline, df, y, cv)\n",
        "    print(f'Baseline accuracy: {baseline_mean:.4f} +/- {baseline_std:.4f}\\n')\n",
        "\n",
        "    # 1) Filter method: SelectKBest (ANOVA f-test)\n",
        "    k = 12\n",
        "    skb = SelectKBest(score_func=f_classif, k=k)\n",
        "    pipeline_skb = Pipeline(steps=[('pre', preprocessor), ('skb', skb), ('clf', clf)])\n",
        "    print(f'Evaluating SelectKBest (f_classif), k={k}...')\n",
        "    skb_mean, skb_std = evaluate_pipeline(pipeline_skb, df, y, cv)\n",
        "    print(f'SelectKBest accuracy: {skb_mean:.4f} +/- {skb_std:.4f}\\n')\n",
        "\n",
        "    # Fit skb to get selected feature names\n",
        "    preprocessed = preprocessor.fit_transform(df)\n",
        "    skb.fit(preprocessed, y)\n",
        "    # Note: ColumnTransformer drops column names; we'll compute scores using preprocessor on dataframe columns\n",
        "    # Simpler: compute scores using raw df columns (since our preprocessor only scales)\n",
        "    skb_feature_indices = skb.get_support(indices=True)\n",
        "    selected_by_skb = [all_features[i] for i in skb_feature_indices]\n",
        "    print('SelectKBest selected features:', selected_by_skb, '\\n')\n",
        "\n",
        "    # 2) Wrapper method: RFE with LogisticRegression\n",
        "    base_lr = LogisticRegression(solver='liblinear', max_iter=200)\n",
        "    rfe = RFE(estimator=base_lr, n_features_to_select=10, step=1)\n",
        "    pipeline_rfe = Pipeline(steps=[('pre', preprocessor), ('rfe', rfe), ('clf', clf)])\n",
        "    print('Evaluating RFE (LogisticRegression) selecting 10 features...')\n",
        "    rfe_mean, rfe_std = evaluate_pipeline(pipeline_rfe, df, y, cv)\n",
        "    print(f'RFE accuracy: {rfe_mean:.4f} +/- {rfe_std:.4f}\\n')\n",
        "\n",
        "    # Fit RFE on full data to get selected features\n",
        "    # Because RFE expects numeric array, transform first\n",
        "    preproc_X = preprocessor.fit_transform(df)\n",
        "    rfe.fit(preproc_X, y)\n",
        "    rfe_support = rfe.get_support()\n",
        "    selected_by_rfe = [all_features[i] for i, keep in enumerate(rfe_support) if keep]\n",
        "    print('RFE selected features:', selected_by_rfe, '\\n')\n",
        "\n",
        "    # 3) Embedded / model-based selection: RandomForest importance -> SelectFromModel\n",
        "    selector = SelectFromModel(estimator=RandomForestClassifier(n_estimators=300, random_state=0), threshold='median')\n",
        "    pipeline_sfm = Pipeline(steps=[('pre', preprocessor), ('sfm', selector), ('clf', clf)])\n",
        "    print('Evaluating SelectFromModel (RandomForest importance, threshold=median) ...')\n",
        "    sfm_mean, sfm_std = evaluate_pipeline(pipeline_sfm, df, y, cv)\n",
        "    print(f'SelectFromModel accuracy: {sfm_mean:.4f} +/- {sfm_std:.4f}\\n')\n",
        "\n",
        "    # Fit selector to get selected features names\n",
        "    selector.fit(preproc_X, y)\n",
        "    sfm_support = selector.get_support()\n",
        "    selected_by_sfm = [all_features[i] for i, keep in enumerate(sfm_support) if keep]\n",
        "    print('SelectFromModel selected features:', selected_by_sfm, '\\n')\n",
        "\n",
        "    # Summary table\n",
        "    results = pd.DataFrame({\n",
        "        'method': ['baseline_all', 'select_kbest', 'rfe', 'select_from_model'],\n",
        "        'mean_accuracy': [baseline_mean, skb_mean, rfe_mean, sfm_mean],\n",
        "        'std_accuracy': [baseline_std, skb_std, rfe_std, sfm_std]\n",
        "    })\n",
        "\n",
        "    print('\\nSummary results:')\n",
        "    print(results.sort_values('mean_accuracy', ascending=False).to_string(index=False))\n",
        "\n",
        "    # Notes for further improvements\n",
        "    print('\\nNotes:')\n",
        "    print('- Try recursive selection + hyperparameter tuning of the estimator for best results.')\n",
        "    print('- Try different feature creation ideas (domain-specific) and interaction terms.')\n",
        "    print('- For high-dimensional data, consider L1-based selection or dimensionality reduction (PCA).')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}